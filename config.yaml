data:
  dataset_root_windows: C:\Users\anaja\OneDrive\Documents\Ecole\TUHH\Semester 6\Masterarbeit\OCT_lab_data # D:\Masterarbeit\OCT_lab_data
  dataset_root_linux: /data/Boudreault/OCT_lab_data # _raw
  ds_split: [0.6, 0.15, 0.25]
  labels:
    - chicken_heart
    - chicken_stomach
    - lamb_heart
    - lamb_testicle
    # - lamb_liver
  ascan_per_group: 5000
  pre_processing:
    no_noise: False
    use_movmean: False
    use_speckle: False
    ascan_sampling: 1 # Set to 1 for no sampling
  use_mini_dataset: True  # Use only files with speed 8 to limit data storage size
  img_mean: 43.51
  img_std: 24.98
training:
  random_seed: 2625233
SPICE:
  MoCo:
    dataset_name: 'oct'  # [stl10, cifar10, cifar100]
    dataset_path_windows: C:\Users\anaja\OneDrive\Documents\Ecole\TUHH\Semester 6\Masterarbeit
    dataset_path_linux: /data/Boudreault
    use_all: False   # True: Use train + test, False: Use only train
    base_model: 'resnet18' # 'clusterresnet'
    save_folder: ./results # Relative path from SPICE/tools or absolute path
    save_freq: 10 # How often the model will be saved
    num_workers: 0 # 8
    max_epochs: 200
    start_epoch: 0  # Useful to restart training
    batch_size: 32
    lr: 0.015
    lr_schedule: [120, 160]  # when to drop lr by 10x
    momentum: 0.9
    weight_decay: 0.0001 # default: 1e-4
    print_freq: 10
    resume: False
    world_size: 1 # number of nodes for distributed training
    rank: 0 # node rank for distributed training
    dist_url: 'env://' # 'env://?use_libuv=False' # 'tcp://localhost:10001' # url used to set up distributed training
    dist_backend: 'nccl' # 'gloo'
    gpu_id: 0 # 0
    multiprocessing_distributed: True
    moco_queue_size: 65536 # 65536 # default: 65535
    moco_momentum: 0.999  # default: 0.999
    moco_softmax_temp: 0.07  # default: 0.07
    moco2_mlp: False
    moco2_aug_plus: False
    moco2_cos: False
  embedding:
    model_name: embedding
    weight: checkpoint_last.pth.tar   # File path: [MoCo save_folder]/[MoCo dataset_name]/[embedding weight]
    batch_size: 32 # 1000
    data_test:
      shuffle: False
      imgs_per_batch: 16 # 50
      aspect_ratio_grouping: False
      train: False
      show: False
    model_sim:
      num_classes: 512 # 128
      in_channels: 3
      batchnorm_track: True
      test: False
      features_only: True
      model_type: "moco_embedding"
  SPICE_self:
    all: 0 # 1
    model_name: spice_self
    resume: False # Start again from ckp
    num_head: 10 # Number of different versions of heads to be trained (best will be kept)
    num_train: 5 # Not used??
    batch_size: 128 # 5000
    target_sub_batch_size: 100
    train_sub_batch_size: 64 # Must be smaller than the batch size
    batch_size_test: 64 # Must be smaller than the batch size
    num_trans_aug: 1 # Not used??
    num_repeat: 8
    fea_dim: 512
    att_size: 7 # Not used??
    center_ratio: 0.5
    sim_center_ratio: 0.9
    epochs: 100
    start_epoch: 0
    print_freq: 2 # Can't be larger than batch_size//sub_batch_size
    test_freq: 1
    eval_ent: False
    eval_ent_weight: 0
  local_consistency:
    model_name: eval
    batch_size: 32 # 100
    batch_size_test: 32
  semi:
    model_name: 'spice_semi'
    resume: False
    overwrite: True
    epoch: 1
    num_train_iter: 2**20
    num_eval_iter: 1000
    num_labels: 6510
    batch_size: 32 # 64
    uratio: 7   # Ratio of unlabeled to labeled data in each mini-batch
    eval_batch_size: 1024
    hard_label: True
    T: 0.5l
    p_cutoff: 0.95
    ema_m: 0.999 # ema momentum for eval model
    ulb_loss_ratio: 1
    lr: 0.03
    momentum: 0.9
    weight_decay: 5e-4
    amp: False  # Use of mixed precision training or not
    net: 'WideResNet_stl10' # ['WideResNet', 'WideResNet_stl10', 'WideResNet_tiny', 'resnet18', 'resnet18_cifar', 'resnet34']
    net_from_name: False
    depth: 28
    widen_factor: 2
    leaky_slope: 0.1
    dropout: 0
    all: 0 # 1
    unlabeled: 0 # 1
    train_sampler: RandomSampler
SimCLR:
  dataset_name: oct  # [stl10, cifar10, cifar100]
  dataset_path_windows: C:\Users\anaja\OneDrive\Documents\Ecole\TUHH\Semester 6\Masterarbeit
  dataset_path_linux: /data/Boudreault
  img_channel: 1
  sample_within_image: 480 # Set to -1 for no sampling
  img_reshape: 256
  use_iipp: True
  num_same_area: 2 # Minimum number of images from the same area that will be used within a batch with iipp
  use_simclr_augmentations: False # If true, use the SimCLR paper transforms. Else, use OCT recommended transforms
  dataset_sample: 0.1  # Sample of the entire dataset
  arch: swinv2t # resnet18, resnet50, vitb16, efficientnetV2s, swinv2t, convnextt, pvtv2b0
  use_pretrained: True
  num_workers: 0
  max_epochs: 200
  batch_size: 16 # 256 # Must be a multiple of num_same_area if iipp is used
  lr: 0.0003
  weight_decay: 0.0001
  disable_cuda: False
  fp16_precision: False
  log_every_n_steps: 2000 # Print update every x batch
  temperature: 0.07 # 0.07 in paper, but 0.5 is often used in code samples
  gpu_index: 0
  patience: 10
BYOL:
  dataset_name: oct  # [stl10, cifar10, cifar100]
  dataset_path_windows: C:\Users\anaja\OneDrive\Documents\Ecole\TUHH\Semester 6\Masterarbeit
  dataset_path_linux: /data/Boudreault
  img_channel: 1
  sample_within_image: 480
  img_reshape: 256
  use_iipp: True
  dataset_sample: 0.1 # 0.25  # Sample of the entire dataset
  arch: pvtv2b0 # resnet18, resnet50, vitb16, efficientnetV2s, swinv2t, convnextt, pvtv2b0
  use_pretrained: True
  num_workers: 0
  max_epochs: 100
  batch_size: 16 # 256
  lr: 0.0003
  disable_cuda: False
  gpu_index: 0
  patience: 10
wandb:
  wandb_log: False
  project_name: 'Test-project' # 'Test-project'  'Mandible-tracking'
  